{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11. NLP (Natural Language Processing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensemble de méthodes informatiques permettant de \"comprendre\" ou analyser le langage naturel, c’est-à-dire notre langage, le langage de l’humain.\n",
    "\n",
    "Un corpus de texte est simplement un ensemble de documents (souvent textuels dans le cas qui nous intéresse).\n",
    "\n",
    "La lexicométrie\n",
    "Science étudiant le texte de manière strictement statistique.\n",
    "\n",
    "Compter les occurrences, et co-occurrences de certains mots, par exemple.\n",
    "\n",
    "Permet de faire l'analyse de larges corpus de texte sans forcément rentrer dans la compréhension du langage lui-même mais plutôt de ses motifs.\n",
    "\n",
    "La présence de certains mots, leurs fréquence et le fait qu'ils co-occurrent avec d'autres nous renseigne, quoique indirectement, sur la nature et le propos du texte."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les N-grammes\n",
    "Méthode strictement mathématique et statistique. (n-grams en anglais)\n",
    "\n",
    "Sous-séquence de n éléments construite à partir d’une séquence donnée.\n",
    "\n",
    "Permet de faire des estimations statistiques bien plus pertinentes sur du texte brut.\n",
    "\n",
    "La probabilité des éléments dans le texte est alors dépendante des éléments précédents et suivants.\n",
    "\n",
    "N-grammes\n",
    "Exemple: mourir de rire vs. mourir de faim.\n",
    "\n",
    "Mourir n'a pas tout à fait la même connotation ici et dans un cas, l'expression véhicule une idée positive et dans l'autre une idée négative.\n",
    "\n",
    "Exemples\n",
    "Bi-grammes, Tri-grammes etc. ",
    "\n",
    "\n",
    "\"123456789\"\n",
    "\"Martin\"\n",
    "\"Cette phrase est un exemple\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La cryptanalyse\n",
    "Analyse lexicométrique du cyphertext afin de percer ses secrets.\n",
    "\n",
    "L'encryption XOR.\n",
    "\n",
    "Comment percer une encryption XOR en ayant que le ciphertext?\n",
    "\n",
    "ETAOIN SHRDLU (rien à voir avec Cthulhu!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le texte brut\n",
    "Le TAL a pour objet le texte brut (données non structurées).\n",
    "\n",
    "Objet d’analyse retors.\n",
    "\n",
    "Comment découper le texte brut en éléments analysables?\n",
    "\n",
    "La tokenization\n",
    "Le POS Tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La tokenization\n",
    "Faire d’un amas de texte brut un amas de tokens (jetons).\n",
    "\n",
    "Découper le texte en phrases (exemple).\n",
    "Découper les phrases en mots.\n",
    "Découper les mots en syllabes.\n",
    "Première étape nécessaire pour le POS tagging.\n",
    "\n",
    "\n",
    "Le Punkt Tokenizer\n",
    "Machine learning non supervisé.\n",
    "\n",
    "Algo de tokénisation de phrases qui prend en entrée la totalité d'un corpus (ou un échantillon représentatif) et entraîne un modèle statistique capable de savoir si, dans du texte, on est en présence d'une fin de phrase ou d'une abbréviation etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le POS tagging\n",
    "Part of Speech tagging (étiquetage morpho-syntaxique en français…)\n",
    "\n",
    "Associer aux mots d'une phrases leurs informations grammaticales.\n",
    "\n",
    "Exemple: Le chat mange la souris.\n",
    "\n",
    "Opération très complexe mais nécessaire lorsqu’il s’agit de tenter de comprendre le sens du texte."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le sentiment analysis\n",
    "Aussi appelé opinion mining.\n",
    "\n",
    "Essayer de déterminer ce que ressentent ou pensent les gens ayant écrit le texte.\n",
    "\n",
    "Exemple: tenter de mesurer si les gens sont plutôt contents ou mécontents lorsqu'ils évoquent la politique du gouvernement sur Twitter.\n",
    "\n",
    "Peu développé scientifiquement. Encore trop heuristique aujourd'hui pour fonctionner de manière suffisamment satisfaisante."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il est extrêmement difficile de trouver des règles tout à fait générales en TAL. Même en combinant des règles à des dictionnaires on n’arrive pas toujours à des solutions satisfaisantes. \n",
    "\n",
    "Aujourd’hui, on utilise donc énormément le machine learning en TAL.\n",
    "\n",
    "On fait de l’apprentissage, en taggant à la main beaucoup des corpora existants et on laisse la machine apprendre d’elle même les règles ou du moins les appliquer sur les corpora suivants.\n",
    "\n",
    "Un perceptron marche par exemple très bien pour créer un POS Tagger. Les arbres de décision aussi.\n",
    "\n",
    "Mais aussi...\n",
    "Parce qu'on ne peut pas tout aborder:\n",
    "\n",
    "Le topic modeling\n",
    "L'extraction de mot-clé\n",
    "L'analyse syntaxique\n",
    "etc.\n",
    "Tout ça pour vous dire qu'on peut faire beaucoup de chose avec \"juste\" du texte."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La logique floue\n",
    "\n",
    "ex : énoncé clair : la voiture roule à 96km/h, énoncé flou : la voiture roule vite.\n",
    "\n",
    "Le fuzzy matching\n",
    "Application primaire de la logique floue: comparer des chaînes de charactères.\n",
    "\n",
    "Intérêts: la recherche floue, la correction orthographique, le clustering.\n",
    "\n",
    "Quatre méthodes canoniques (à combiner): ",
    " ",
    "\n",
    "\n",
    "La normalisation\n",
    "Les distances mathématiques ",
    "\n",
    "La lemmatisation/racinisation ",
    "\n",
    "Les algorithmes phonétiques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La normalisation\n",
    "Dupond & dupond.\n",
    "\n",
    "Normaliser une chaîne de charactère pour faciliter les comparaisons.\n",
    "\n",
    "Harmoniser la casse\n",
    "Supprimer les accents\n",
    "Harmoniser les charactères unicodes (guillemets, apostrophes)\n",
    "Elaguer les chaînes\n",
    "Compresser les espaces\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorisation d'une phrase (voir BOW, CBOW, TF-IDF)\n",
    "\n",
    "'Le chat mange la souris.'\n",
    "'La souris mange le fromage.'\n",
    "\n",
    "{le: 1, chat: 2, mange: 3, la: 4, souris: 5, fromage: 6}\n",
    "\n",
    "[1, 1, 1, 1, 1, 0]\n",
    "[1, 0, 1, 1, 1, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –– Sommaire\n",
    "\n",
    "### A. NLTK\n",
    "\n",
    "• Pré-traitement du texte  \n",
    "• Traitement du texte  \n",
    "\n",
    "### B. Modélisation automatique de sujet\n",
    "\n",
    "• LDA  \n",
    "• NMF  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –– BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rang</th>\n",
       "      <th>Pseudo</th>\n",
       "      <th>Plateforme</th>\n",
       "      <th>Nombre de victoires</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>JeDiiiKniiGhT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>AlexRamiGaming</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Erouce</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>SypherPK</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>semm1234</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>KingRichard215</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>Loeya</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>Keepo_</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>Eragonist</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>Fnatic_Eryc</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>Ninja</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>LeG3nDz</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>twitchtvLIKANDOO</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>Dark</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>FaZe Jaomock</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>TaDeNight</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>Strandlii</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>DanzhizzLe</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>Nick Eh 30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>Z_E_A_P</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>Kiryse1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>ItsMacau</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>Leonaise</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>Avxry</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>FaZe Tennp0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26</td>\n",
       "      <td>Twitch_Aphostle</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27</td>\n",
       "      <td>Symfuhny</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>28</td>\n",
       "      <td>Vanes130</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>29</td>\n",
       "      <td>Zaccubus</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>30</td>\n",
       "      <td>joethebroo</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>71</td>\n",
       "      <td>YTExempIe</td>\n",
       "      <td>NaN</td>\n",
       "      <td>779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>72</td>\n",
       "      <td>drnkie</td>\n",
       "      <td>NaN</td>\n",
       "      <td>779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>73</td>\n",
       "      <td>Nicania</td>\n",
       "      <td>NaN</td>\n",
       "      <td>771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>74</td>\n",
       "      <td>LtRyan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>75</td>\n",
       "      <td>Getboxy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>76</td>\n",
       "      <td>Blz_Vince_91</td>\n",
       "      <td>NaN</td>\n",
       "      <td>760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>77</td>\n",
       "      <td>Neixaa</td>\n",
       "      <td>NaN</td>\n",
       "      <td>754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>78</td>\n",
       "      <td>ChaosZTR</td>\n",
       "      <td>NaN</td>\n",
       "      <td>747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>79</td>\n",
       "      <td>VP JAMSIDE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>80</td>\n",
       "      <td>DrLupo</td>\n",
       "      <td>NaN</td>\n",
       "      <td>735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>81</td>\n",
       "      <td>BanishedOnTwitch</td>\n",
       "      <td>NaN</td>\n",
       "      <td>724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>82</td>\n",
       "      <td>Kraftyyz</td>\n",
       "      <td>NaN</td>\n",
       "      <td>720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>83</td>\n",
       "      <td>Disypal</td>\n",
       "      <td>NaN</td>\n",
       "      <td>705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>84</td>\n",
       "      <td>dolu113</td>\n",
       "      <td>NaN</td>\n",
       "      <td>705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>85</td>\n",
       "      <td>HycrisZ1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>86</td>\n",
       "      <td>Valstirk.TV</td>\n",
       "      <td>NaN</td>\n",
       "      <td>696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>87</td>\n",
       "      <td>PopiGames</td>\n",
       "      <td>NaN</td>\n",
       "      <td>694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>88</td>\n",
       "      <td>DatHunterooMain</td>\n",
       "      <td>NaN</td>\n",
       "      <td>690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>89</td>\n",
       "      <td>100T Kenith</td>\n",
       "      <td>NaN</td>\n",
       "      <td>687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>90</td>\n",
       "      <td>GSTARZZ</td>\n",
       "      <td>NaN</td>\n",
       "      <td>683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>91</td>\n",
       "      <td>Kalzzon</td>\n",
       "      <td>NaN</td>\n",
       "      <td>682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>92</td>\n",
       "      <td>PonchoPanch</td>\n",
       "      <td>NaN</td>\n",
       "      <td>682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>93</td>\n",
       "      <td>Kroatomist</td>\n",
       "      <td>NaN</td>\n",
       "      <td>674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>94</td>\n",
       "      <td>Gowtyx</td>\n",
       "      <td>NaN</td>\n",
       "      <td>667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>95</td>\n",
       "      <td>Dizo San</td>\n",
       "      <td>NaN</td>\n",
       "      <td>659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>96</td>\n",
       "      <td>Twitch NowardBE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>97</td>\n",
       "      <td>Never_back_R</td>\n",
       "      <td>NaN</td>\n",
       "      <td>653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>98</td>\n",
       "      <td>xDmZzI</td>\n",
       "      <td>NaN</td>\n",
       "      <td>652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>99</td>\n",
       "      <td>TwitchRazZzero0o</td>\n",
       "      <td>NaN</td>\n",
       "      <td>634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>100</td>\n",
       "      <td>GUCCIIZIPUNTAKA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>632</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Rang            Pseudo  Plateforme  Nombre de victoires\n",
       "0      1     JeDiiiKniiGhT         NaN                 4702\n",
       "1      2    AlexRamiGaming         NaN                 4352\n",
       "2      3            Erouce         NaN                 3778\n",
       "3      4          SypherPK         NaN                 3048\n",
       "4      5          semm1234         NaN                 2898\n",
       "5      6    KingRichard215         NaN                 2539\n",
       "6      7             Loeya         NaN                 2455\n",
       "7      8            Keepo_         NaN                 2116\n",
       "8      9         Eragonist         NaN                 2038\n",
       "9     10       Fnatic_Eryc         NaN                 2016\n",
       "10    11             Ninja         NaN                 1922\n",
       "11    12           LeG3nDz         NaN                 1873\n",
       "12    13  twitchtvLIKANDOO         NaN                 1781\n",
       "13    14              Dark         NaN                 1748\n",
       "14    15      FaZe Jaomock         NaN                 1747\n",
       "15    16         TaDeNight         NaN                 1636\n",
       "16    17         Strandlii         NaN                 1622\n",
       "17    18        DanzhizzLe         NaN                 1499\n",
       "18    19        Nick Eh 30         NaN                 1492\n",
       "19    20           Z_E_A_P         NaN                 1492\n",
       "20    21           Kiryse1         NaN                 1457\n",
       "21    22          ItsMacau         NaN                 1452\n",
       "22    23          Leonaise         NaN                 1440\n",
       "23    24             Avxry         NaN                 1430\n",
       "24    25       FaZe Tennp0         NaN                 1427\n",
       "25    26   Twitch_Aphostle         NaN                 1406\n",
       "26    27          Symfuhny         NaN                 1392\n",
       "27    28          Vanes130         NaN                 1390\n",
       "28    29          Zaccubus         NaN                 1372\n",
       "29    30        joethebroo         NaN                 1330\n",
       "..   ...               ...         ...                  ...\n",
       "70    71         YTExempIe         NaN                  779\n",
       "71    72            drnkie         NaN                  779\n",
       "72    73           Nicania         NaN                  771\n",
       "73    74            LtRyan         NaN                  768\n",
       "74    75           Getboxy         NaN                  768\n",
       "75    76      Blz_Vince_91         NaN                  760\n",
       "76    77            Neixaa         NaN                  754\n",
       "77    78          ChaosZTR         NaN                  747\n",
       "78    79        VP JAMSIDE         NaN                  738\n",
       "79    80            DrLupo         NaN                  735\n",
       "80    81  BanishedOnTwitch         NaN                  724\n",
       "81    82          Kraftyyz         NaN                  720\n",
       "82    83           Disypal         NaN                  705\n",
       "83    84           dolu113         NaN                  705\n",
       "84    85          HycrisZ1         NaN                  701\n",
       "85    86       Valstirk.TV         NaN                  696\n",
       "86    87         PopiGames         NaN                  694\n",
       "87    88   DatHunterooMain         NaN                  690\n",
       "88    89       100T Kenith         NaN                  687\n",
       "89    90           GSTARZZ         NaN                  683\n",
       "90    91           Kalzzon         NaN                  682\n",
       "91    92       PonchoPanch         NaN                  682\n",
       "92    93        Kroatomist         NaN                  674\n",
       "93    94            Gowtyx         NaN                  667\n",
       "94    95          Dizo San         NaN                  659\n",
       "95    96   Twitch NowardBE         NaN                  657\n",
       "96    97      Never_back_R         NaN                  653\n",
       "97    98            xDmZzI         NaN                  652\n",
       "98    99  TwitchRazZzero0o         NaN                  634\n",
       "99   100   GUCCIIZIPUNTAKA         NaN                  632\n",
       "\n",
       "[100 rows x 4 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "res = requests.get(\"https://tracker.fortnitefrance.eu/leaderboards\")\n",
    "soup = BeautifulSoup(res.content,'lxml')\n",
    "table = soup.find_all('table')[0]\n",
    "df = pd.read_html(str(table))[0]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>place</th>\n",
       "      <th>username</th>\n",
       "      <th>xp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1st</td>\n",
       "      <td>Hidro</td>\n",
       "      <td>81553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2nd</td>\n",
       "      <td>Baker</td>\n",
       "      <td>78608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3rd</td>\n",
       "      <td>Gunservative</td>\n",
       "      <td>76188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4th</td>\n",
       "      <td>Mikey</td>\n",
       "      <td>51208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5th</td>\n",
       "      <td>Tanntrix</td>\n",
       "      <td>49011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6th</td>\n",
       "      <td>ReconsPOV</td>\n",
       "      <td>45085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7th</td>\n",
       "      <td>x2Pac ThuGLorD</td>\n",
       "      <td>42073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8th</td>\n",
       "      <td>Rules</td>\n",
       "      <td>41415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9th</td>\n",
       "      <td>Rallied</td>\n",
       "      <td>40188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10th</td>\n",
       "      <td>Olly</td>\n",
       "      <td>39282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11th</td>\n",
       "      <td>TKhan</td>\n",
       "      <td>38292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12th</td>\n",
       "      <td>MuTeX</td>\n",
       "      <td>35337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13th</td>\n",
       "      <td>stridox</td>\n",
       "      <td>34939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14th</td>\n",
       "      <td>Innocent</td>\n",
       "      <td>34897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15th</td>\n",
       "      <td>ihavok</td>\n",
       "      <td>34771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16th</td>\n",
       "      <td>TMTwerk</td>\n",
       "      <td>34275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17th</td>\n",
       "      <td>exoficial</td>\n",
       "      <td>34057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18th</td>\n",
       "      <td>iLLeY</td>\n",
       "      <td>33928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19th</td>\n",
       "      <td>C</td>\n",
       "      <td>33096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20th</td>\n",
       "      <td>UsernameChange12911</td>\n",
       "      <td>32795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21st</td>\n",
       "      <td>Monstrous</td>\n",
       "      <td>32637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22nd</td>\n",
       "      <td>TeddyRecKs</td>\n",
       "      <td>32248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23rd</td>\n",
       "      <td>Trichoblast</td>\n",
       "      <td>32165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24th</td>\n",
       "      <td>Saggio</td>\n",
       "      <td>31629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25th</td>\n",
       "      <td>iBuLieVe</td>\n",
       "      <td>31291</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   place             username     xp\n",
       "0    1st                Hidro  81553\n",
       "1    2nd                Baker  78608\n",
       "2    3rd         Gunservative  76188\n",
       "3    4th                Mikey  51208\n",
       "4    5th             Tanntrix  49011\n",
       "5    6th            ReconsPOV  45085\n",
       "6    7th       x2Pac ThuGLorD  42073\n",
       "7    8th                Rules  41415\n",
       "8    9th              Rallied  40188\n",
       "9   10th                 Olly  39282\n",
       "10  11th                TKhan  38292\n",
       "11  12th                MuTeX  35337\n",
       "12  13th              stridox  34939\n",
       "13  14th             Innocent  34897\n",
       "14  15th               ihavok  34771\n",
       "15  16th              TMTwerk  34275\n",
       "16  17th            exoficial  34057\n",
       "17  18th                iLLeY  33928\n",
       "18  19th                    C  33096\n",
       "19  20th  UsernameChange12911  32795\n",
       "20  21st            Monstrous  32637\n",
       "21  22nd           TeddyRecKs  32248\n",
       "22  23rd          Trichoblast  32165\n",
       "23  24th               Saggio  31629\n",
       "24  25th             iBuLieVe  31291"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "#Get the data\n",
    "data = requests.get('https://umggaming.com/leaderboards')\n",
    "\n",
    "#Give data to BeautifulSoup\n",
    "soup = BeautifulSoup(data.text, 'html.parser')\n",
    "\n",
    "leaderboard = soup.find('table', { 'id': 'leaderboard-table' })\n",
    "tbody = leaderboard.find('tbody')\n",
    "\n",
    "place = []\n",
    "username = []\n",
    "xp = []\n",
    "\n",
    "for tr in tbody.find_all('tr'):\n",
    "    place.append(tr.find_all('td')[0].text.strip())\n",
    "    username.append(tr.find_all('td')[1].find_all('a')[1].text.strip())\n",
    "    xp.append(tr.find_all('td')[3].text.strip())\n",
    "\n",
    "import pandas as pd\n",
    "d = {'place': place, 'username': username, 'xp': xp}\n",
    "df = pd.DataFrame(d)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –– Pré-traitement du texte\n",
    "\n",
    "- **Récupération du corpus** par scraping ou en téléchargeant des fichiers textes, par exemple. Cela peut demander l'utilisation de regex afin de récupérer uniquement les parties qui vous intéressent.\n",
    "- **La tokenization**, qui désigne le découpage en mots des différents documents qui constituent votre corpus\n",
    "- **La normalisation et la construction du dictionnaire** qui permet de ne pas prendre en compte des détails importants au niveau local (ponctuation, majuscules, conjugaison, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/quentinmonmousseau/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Bonjour', ',', 'je', 'suis', 'un', 'texte', \"d'exemple\", 'pour', 'la', '#', 'démonstration', '!', '!', '!']\n",
      "['Bonjour', 'je', 'suis', 'un', 'texte', 'd', 'exemple', 'pour', 'la', 'démonstration']\n"
     ]
    }
   ],
   "source": [
    "#Récupération du corpus\n",
    "corpus = \"Bonjour, je suis un texte d'exemple pour la #démonstration !!!\"\n",
    "\n",
    "#Tokenization en tableau de mots\n",
    "print(nltk.word_tokenize(corpus))\n",
    "\n",
    "#Normalisation\n",
    "tokenizer = nltk.RegexpTokenizer(r'\\w+')\n",
    "print(tokenizer.tokenize(corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –– Traîtement du texte\n",
    "\n",
    "- **Suppression des stopwords** qui sont les mots très courants dans la langue étudiée (\"et\", \"à\", \"le\"... en français) qui n'apportent pas de valeur informative pour la compréhension du \"sens\" d'un document.\n",
    "- **Suppression des mots les plus fréquents** pour ne garder que ceux qui permettent de distinguer le contenu d'un texte d'un autre.\n",
    "- **Stemming** qui permet de supprimer les préfixes/suffixes pour ne garder que la racine (la partie la plus porteuse de sens) des mots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. Modélisation automatique de sujet\n",
    "\n",
    "L'objectif de ce type de modélisation de sujets est de récupérer de potentielles catégories pour des traitements ultérieurs. Cette modélisation offre surtout une meilleure compréhension de la structuration du texte en vue de création de features manuelles (mettre l'accent sur certains mots, comprendre ce qui définit une catégorie, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –– LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prétraitement\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "dataset = fetch_20newsgroups(shuffle=True, \n",
    "                             random_state=1, \n",
    "                             remove=('headers', 'footers', 'quotes')\n",
    "                            )\n",
    "documents = dataset.data\n",
    "\n",
    "#Traitement\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "tf_vectorizer = CountVectorizer(max_df=0.95,\n",
    "                                min_df=2,\n",
    "                                max_features=1000,\n",
    "                                stop_words='english'\n",
    "                               )\n",
    "\n",
    "tf = tf_vectorizer.fit_transform(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Modélisation automatique de sujet avec le modèle LDA\n",
    "\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "nb_topics = 20\n",
    "lda = LatentDirichletAllocation(n_components=nb_topics,\n",
    "                                max_iter=5,\n",
    "                                learning_method='online',\n",
    "                                learning_offset=50.,\n",
    "                                random_state=0\n",
    "                               ).fit(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "people gun state control right guns crime states law police\n",
      "Topic 1:\n",
      "time question book years did like don space answer just\n",
      "Topic 2:\n",
      "mr line rules science stephanopoulos title current define int yes\n",
      "Topic 3:\n",
      "key chip keys clipper encryption number des algorithm use bit\n",
      "Topic 4:\n",
      "edu com cs vs w7 cx mail uk 17 send\n",
      "Topic 5:\n",
      "use does window problem way used point different case value\n",
      "Topic 6:\n",
      "windows thanks know help db does dos problem like using\n",
      "Topic 7:\n",
      "bike water effect road design media dod paper like turn\n",
      "Topic 8:\n",
      "don just like think know people good ve going say\n",
      "Topic 9:\n",
      "car new price good power used air sale offer ground\n",
      "Topic 10:\n",
      "file available program edu ftp information files use image version\n",
      "Topic 11:\n",
      "ax max b8f g9v a86 145 pl 1d9 0t 34u\n",
      "Topic 12:\n",
      "government law privacy security legal encryption court fbi technology information\n",
      "Topic 13:\n",
      "card bit memory output video color data mode monitor 16\n",
      "Topic 14:\n",
      "drive scsi disk mac hard apple drives controller software port\n",
      "Topic 15:\n",
      "god jesus people believe christian bible say does life church\n",
      "Topic 16:\n",
      "year game team games season play hockey players league player\n",
      "Topic 17:\n",
      "10 00 15 25 20 11 12 14 16 13\n",
      "Topic 18:\n",
      "armenian israel armenians war people jews turkish israeli said women\n",
      "Topic 19:\n",
      "president people new said health year university school day work\n"
     ]
    }
   ],
   "source": [
    "#Display\n",
    "def display_topics(model, feature_names, nb_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic %d:\" % (topic_idx))\n",
    "        print(\" \".join([feature_names[i] for i in topic.argsort()[:-nb_top_words - 1:-1]]))\n",
    "\n",
    "nb_top_words = 10\n",
    "display_topics(lda, tf_vectorizer.get_feature_names(), nb_top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –– NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "people time right did good said say make way government\n",
      "Topic 1:\n",
      "window problem using server application screen display motif manager running\n",
      "Topic 2:\n",
      "god jesus bible christ faith believe christian christians sin church\n",
      "Topic 3:\n",
      "game team year games season players play hockey win league\n",
      "Topic 4:\n",
      "new 00 sale 10 price offer shipping condition 20 15\n",
      "Topic 5:\n",
      "thanks mail advance hi looking info help information address appreciated\n",
      "Topic 6:\n",
      "windows file files dos program version ftp ms directory running\n",
      "Topic 7:\n",
      "edu soon cs university ftp internet article email pub david\n",
      "Topic 8:\n",
      "key chip clipper encryption keys escrow government public algorithm nsa\n",
      "Topic 9:\n",
      "drive scsi drives hard disk ide floppy controller cd mac\n",
      "Topic 10:\n",
      "just ll thought tell oh little fine work wanted mean\n",
      "Topic 11:\n",
      "does know anybody mean work say doesn help exist program\n",
      "Topic 12:\n",
      "card video monitor cards drivers bus vga driver color memory\n",
      "Topic 13:\n",
      "like sounds looks look bike sound lot things really thing\n",
      "Topic 14:\n",
      "don know want let need doesn little sure sorry things\n",
      "Topic 15:\n",
      "car cars engine speed good bike driver road insurance fast\n",
      "Topic 16:\n",
      "ve got seen heard tried good recently times try couple\n",
      "Topic 17:\n",
      "use used using work available want software need image data\n",
      "Topic 18:\n",
      "think don lot try makes really pretty wasn bit david\n",
      "Topic 19:\n",
      "com list dave internet article sun hp email ibm phone\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "# NMF is able to use tf-idf\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, max_features=1000, stop_words='english')\n",
    "tfidf = tfidf_vectorizer.fit_transform(documents)\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "\n",
    "# Run NMF\n",
    "nmf = NMF(n_components=20, random_state=1, alpha=.1, l1_ratio=.5, init='nndsvd').fit(tfidf)\n",
    "\n",
    "no_top_words = 10\n",
    "display_topics(nmf, tfidf_feature_names, no_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **livrable OCR** : Vous devrez effectuer les opérations de traitement suivantes sur le texte, pas forcément dans cet ordre\n",
    "\n",
    "> - Créer des paires de document (article, highlights)\n",
    "- Suppression de la ponctuation\n",
    "- Séparation en token en minuscules\n",
    "- Suppression des stopwords pour les articles\n",
    "- Calcul des fréquences et tf-idf sur les deux types de documents\n",
    "- Enregistrement du nouveau jeu de données d’entraînement pour usage ultérieur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
